<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="In 2014/2015, it took NVIDIA 6 months from the launch of the Maxwell 2 architecture to get GTX Titan X out the door. All things considered, that was a fast turnaround for a new architecture. However now that were the Pascal generation, it turns out NVIDIA is in the mood to set a speed record,"><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Updated: NVIDIA Announces NVIDIA Titan X Video Card: $1200, Available August 2nd</title><link rel=canonical href=./nvidia-announces-nvidia-titan-x-video-card-1200-available-august-2nd.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>JazzBlog</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Updated: NVIDIA Announces NVIDIA Titan X Video Card: $1200, Available August 2nd</h1><div id=sub-header>April 2024 · 8 minute read</div><div class=entry-content><p>In 2014/2015, it took NVIDIA 6 months from the launch of the Maxwell 2 architecture to get GTX Titan X out the door. All things considered, that was a fast turnaround for a new architecture. However now that we’re the Pascal generation, it turns out NVIDIA is in the mood to set a speed record, and in more ways than one.</p><p>Announced this evening by Jen-Hsun Huang at an engagement at Stanford University is the NVIDIA Titan X, NVIDIA’s new flagship video card. Based on the company’s new GP102 GPU, it’s launching in less than two weeks, on August 2nd.</p><table align=center border=1 cellpadding=3 cellspacing=0 width=650><tbody readability=1><tr class=tgrey readability=2><td align=center colspan=7>NVIDIA GPU Specification Comparison</td></tr><tr class=tlblue><td align=center bgcolor=#016a96 class=contentwhite width=109>&nbsp;</td><td align=center bgcolor=#016a96 class=contentwhite width=85>NVIDIA Titan X</td><td align=center bgcolor=#016a96 class=contentwhite width=85>GTX 1080</td><td align=center bgcolor=#016a96 class=contentwhite width=85>GTX Titan X</td><td align=center bgcolor=#016a96 class=contentwhite width=85>GTX Titan</td></tr><tr><td class=tlgrey><strong>CUDA Cores</strong></td><td align=center bgcolor=#f7f7f7>3584</td><td align=center bgcolor=#f7f7f7>2560</td><td align=center bgcolor=#f7f7f7>3072</td><td align=center bgcolor=#f7f7f7>2688</td></tr><tr><td class=tlgrey><strong>Texture Units</strong></td><td align=center bgcolor=#f7f7f7>224?</td><td align=center bgcolor=#f7f7f7>160</td><td align=center bgcolor=#f7f7f7>192</td><td align=center bgcolor=#f7f7f7>224</td></tr><tr><td class=tlgrey><strong>ROPs</strong></td><td align=center bgcolor=#f7f7f7>96?</td><td align=center bgcolor=#f7f7f7>64</td><td align=center bgcolor=#f7f7f7>96</td><td align=center bgcolor=#f7f7f7>48</td></tr><tr><td class=tlgrey><strong>Core Clock</strong></td><td align=center bgcolor=#f7f7f7>1417MHz</td><td align=center bgcolor=#f7f7f7>1607MHz</td><td align=center bgcolor=#f7f7f7>1000MHz</td><td align=center bgcolor=#f7f7f7>837MHz</td></tr><tr><td class=tlgrey><strong>Boost Clock</strong></td><td align=center bgcolor=#f7f7f7>1531MHz</td><td align=center bgcolor=#f7f7f7>1733MHz</td><td align=center bgcolor=#f7f7f7>1075MHz</td><td align=center bgcolor=#f7f7f7>876MHz</td></tr><tr><td class=tlgrey><strong>TFLOPs (FMA)</strong></td><td align=center bgcolor=#f7f7f7>11 TFLOPs</td><td align=center bgcolor=#f7f7f7>9 TFLOPs</td><td align=center bgcolor=#f7f7f7>6.6 TFLOPs</td><td align=center bgcolor=#f7f7f7>4.7 TFLOPs</td></tr><tr><td class=tlgrey><strong>Memory Clock</strong></td><td align=center bgcolor=#f7f7f7>10Gbps GDDR5X</td><td align=center bgcolor=#f7f7f7>10Gbps GDDR5X</td><td align=center bgcolor=#f7f7f7>7Gbps GDDR5</td><td align=center bgcolor=#f7f7f7>6Gbps GDDR5</td></tr><tr><td class=tlgrey><strong>Memory Bus Width</strong></td><td align=center bgcolor=#f7f7f7>384-bit</td><td align=center bgcolor=#f7f7f7>256-bit</td><td align=center bgcolor=#f7f7f7>384-bit</td><td align=center bgcolor=#f7f7f7>384-bit</td></tr><tr><td class=tlgrey><strong>VRAM</strong></td><td align=center bgcolor=#f7f7f7>12GB</td><td align=center bgcolor=#f7f7f7>8GB</td><td align=center bgcolor=#f7f7f7>12GB</td><td align=center bgcolor=#f7f7f7>6GB</td></tr><tr><td class=tlgrey><strong>FP64</strong></td><td align=center bgcolor=#f7f7f7>1/32</td><td align=center bgcolor=#f7f7f7>1/32</td><td align=center bgcolor=#f7f7f7>1/32</td><td align=center bgcolor=#f7f7f7>1/3</td></tr><tr><td class=tlgrey><strong>FP16 (Native)</strong></td><td align=center bgcolor=#f7f7f7>1/64</td><td align=center bgcolor=#f7f7f7>1/64</td><td align=center bgcolor=#f7f7f7>N/A</td><td align=center bgcolor=#f7f7f7>N/A</td></tr><tr><td class=tlgrey><strong>INT8</strong></td><td align=center bgcolor=#f7f7f7>4:1</td><td align=center bgcolor=#f7f7f7>?</td><td align=center bgcolor=#f7f7f7>?</td><td align=center bgcolor=#f7f7f7>?</td></tr><tr><td class=tlgrey><strong>TDP</strong></td><td align=center bgcolor=#f7f7f7>250W</td><td align=center bgcolor=#f7f7f7>180W</td><td align=center bgcolor=#f7f7f7>250W</td><td align=center bgcolor=#f7f7f7>250W</td></tr><tr><td class=tlgrey><strong>GPU</strong></td><td align=center bgcolor=#f7f7f7>GP102</td><td align=center bgcolor=#f7f7f7>GP104</td><td align=center bgcolor=#f7f7f7>GM200</td><td align=center bgcolor=#f7f7f7>GK110</td></tr><tr><td class=tlgrey><strong>Transistor Count</strong></td><td align=center bgcolor=#f7f7f7>12B</td><td align=center bgcolor=#f7f7f7>7.2B</td><td align=center bgcolor=#f7f7f7>8B</td><td align=center bgcolor=#f7f7f7>7.1B</td></tr><tr><td class=tlgrey><strong>Die Size</strong></td><td align=center bgcolor=#f7f7f7>471mm2</td><td align=center bgcolor=#f7f7f7>314mm2</td><td align=center bgcolor=#f7f7f7>601mm2</td><td align=center bgcolor=#f7f7f7>551mm2</td></tr><tr><td class=tlgrey><strong>Manufacturing Process</strong></td><td align=center bgcolor=#f7f7f7>TSMC 16nm</td><td align=center bgcolor=#f7f7f7>TSMC 16nm</td><td align=center bgcolor=#f7f7f7>TSMC 28nm</td><td align=center bgcolor=#f7f7f7>TSMC 28nm</td></tr><tr><td class=tlgrey><strong>Launch Date</strong></td><td align=center bgcolor=#f7f7f7>08/02/2016</td><td align=center bgcolor=#f7f7f7>05/27/2016</td><td align=center bgcolor=#f7f7f7>03/17/2015</td><td align=center bgcolor=#f7f7f7>02/21/2013</td></tr><tr><td class=tlgrey><strong>Launch Price</strong></td><td align=center bgcolor=#f7f7f7>$1200</td><td align=center bgcolor=#f7f7f7>MSRP: $599<br>Founders $699</td><td align=center bgcolor=#f7f7f7>$999</td><td align=center bgcolor=#f7f7f7>$999</td></tr></tbody></table><p>Let’s dive right into the numbers, shall we? The NVIDIA Titan X will be shipping with 3584 CUDA cores. Assuming that NVIDIA retains their GP104-style consumer architecture here – and there’s every reason to expect they will – then we’re looking at 28 SMs, or 40% more than GP104 and the GTX 1080.</p><p>It’s interesting to note here that 3584 CUDA cores happens to be the exact same number of CUDA cores also found in the <a href=#>Tesla P100 accelerator</a>. These products are based on very different GPUs, but I bring this up because Tesla P100 did not use a fully enabled GP100 GPU; its GPU features 3840 CUDA cores in total. NVIDIA is not confirming the total number of CUDA cores in GP102 at this time, but if it’s meant to be a lightweight version of GP100, then this may not be a fully enabled card. This would also maintain the 3:2:1 ratio between GP102/104/106, as we saw with GM200/204/206.</p><p>On the clockspeed front, Titan X will be clocked at 1417MHz base and 1531MHz boost. This puts the total FP32 throughput at 11 TFLOPs (well, 10.97…), 24% higher than GTX 1080. In terms of expected performance, NVIDIA isn’t offering any comparisons to GTX 1080 at this time, but relative to the Maxwell 2 based GTX Titan X, they are talking about an up to 60% performance boost.</p><p>Feeding the beast that is GP102 is a 384-bit GDDR5X memory bus. NVIDIA will be running Titan X’s GDDR5X at the same 10Gbps as on GTX 1080, so we’re looking at a straight-up 50% increase in memory bus size and resulting memory bandwidth, bringing Titan X to 480GB/sec.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10510/JHHTX_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>At this point in time there are a few unknowns about other specifications of the card. ROP count and texture unit count have not been disclosed (and this is something NVIDIA rarely posts on their site anyhow), but based on GP104 and GP106, I believe it’s safe to assume that we’re looking at 224 texture units and 96 ROPs respectively. To put this into numbers then, theoretical performance versus a GTX 1080 would be 24% more shading/texturing/geometry/compute performance, 50% more memory bandwidth, and 33% more ROP throughput. Or relative GTX Titan X (Maxwell 2), 56% more shading/texturing/geometry/compute performance, 43% more memory bandwidth, and 42% more ROP throughput. Of course, none of this takes into account any of Pascal’s architectural advantages such as a new delta color compression system.</p><p>Meanwhile like the past Titans, the new Titan X is a 250W card, putting it 70W (39%) above GTX 1080. In pictures released by NVIDIA and confirmed by their spec sheet, this will be powered by the typical 8-pin + 6-pin power connector setup. And speaking of pictures, the handful of pictures released so far confirm that the card will be following NVIDIA’s previous reference design, in the new GTX 1000 series triangular style. This means we’re looking at a blower based card – now clad in black for Titan X – using a vapor chamber setup like the GTX 1080 and past Titan cards.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/10510/nvidia-titan-x-pascal-003_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The TDP difference between Titan X and GTX 1080 may also explain some of rationale behind the performance estimates above. In the Maxwel 2 generation, GTX Titan X (250W) consumed 85W more than GTX 980 (165W); but for the Pascal generation, NVIDIA only gets another 70W. As power is the ultimate factor limiting performance, it stands to reason that NVIDIA can't increase performance over GTX 1080 (in the form of CUDA cores and clockspeeds) by as much as they could over GTX 980. There is always the option to go above 250W - Tesla P100 in mezzanine form goes to 300 W - but for a PCIe form factor, 250W seems to be the sweet spot for NVIDIA.</p><p>Moving on, display I/O is listed as DisplayPort 1.4, HDMI 2.0b, and DL-DVI; NVIDIA doesn’t list the number of ports (and they aren’t visible in product photos), but I’d expect that it’s 3x DP, 1x HDMI, and 1x DL-DVI, just as with the past Titan X and GTX 1080.</p><p>From a marketing standpoint, it goes without saying that NVIDIA is pitching the Titan X as their new flagship card. What is interesting however is that it’s not being classified as a GeForce card, rather it’s the amorphous “NVIDIA Titan X”, being neither Quadro, Tesla, nor GeForce. Since the first card’s introduction in 2013, the GTX Titan series has always walked a fine line as a prosumer card, balanced between a relatively cheap compute card for workstations, and an uber gaming card for gaming PCs.</p><p>That NVIDIA has removed this card from the GeForce family would seem to further cement its place as a prosumer card. On the compute front the company is separately advertising the card's 44 TOPs INT8 compute performance - INT8 being frequently used for neural network inference - which is something they haven't done before for GeForce or Titan cards. Though make no mistake: the company’s <a href=#>GeForce division is marketing the card</a> and it’s listed on GeForce.com, so it is still very much a gaming card as well.</p><p>As for pricing and availability, NVIDIA’s flagships have always been expensive, and NVIDIA Titan X even more so. The card will retail for $1200, $200 more than the previous GTX Titan X (Maxwell 2), and $500 more than the NVIDIA-built GTX 1080 Founders Edition. Given the overall higher prices for the GTX 1000 series, this isn’t something that surprises me, but none the less it means buying NVIDIA’s best card just got a bit more expensive. Meanwhile for distribution, making a departure from previous generations, the card is only being sold directly by NVIDIA through their website. The company’s board partners will not be distributing it, though system builders will still be able to include it.</p><p>Overall the announcement of this new Titan card, its specifications, and its timing raises a lot of questions. Does GP102 have fast FP64/FP16 hardware, or is it purely a larger GP104, finally formalizing the long-anticipated divide between HPC and consumer GPUs? Just how much smaller is GP102 versus GP100? How has NVIDIA been able to contract their launch window by so much for the Pascal generation, launching 3 GPUs in the span of 3 months? These are all good questions I hope we’ll get an answer to, and with an August 2nd launch it looks like we won’t be waiting too long.</p><p><strong>Update 07/25</strong>: NVIDIA has given us a few answers to the question above. We have confirmation that the FP64 and FP16 rates are identical to GP104, which is to say very slow, and primarily there for compatibility/debug purposes. With the exception of INT8 support, this is a bigger GP104 throughout.</p><p>Meanwhile we have a die size for GP102: 471mm2, which is 139mm2 smaller than GP100. Given that both (presumably) have the same number of FP32 cores, the die space savings and implications are significant. This is as best of an example as we're ever going to get on the die space cost of the HPC features limited to GP100: NVLInk, fast FP64/FP16 support, larger register files, etc. By splitting HPC and graphics/inference into two GPUs, NVIDIA can produce GP102 at what should be a significantly lower price (and higher yield), something they couldn't do until the market for compute products based on GP100 was self-sustaining.</p><p>Finally, NVIDIA has clarified the branding a bit. Despite GeForce.com labeling it "the world’s ultimate graphics card," NVIDIA this morning has stated that the primary market is FP32 and INT8 compute, not gaming. Though gaming is certainly possible - and I fully expect they'll be happy to sell you $1200 gaming cards - the tables have essentially been flipped from the past Titan cards, where they were treated as gaming first and compute second. This of course opens the door to a proper GeForce branded GP102 card later on, possibly with neutered INT8 support to enforce the market segmentation.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH5xgZBpZqeumZm2onnAp6WorZ6YsrR5za%2BgnaGRYsGqwMCnZLFlpp6xpruMnJirnF1mf3F8jJqtmqGclq%2BtsYyarKCto6l6c7rD</p></div><div id=links><a href=./what-happened-to-ruby-wax-illness-and-health-update.html>&#171;&nbsp;What Happened To Ruby Wax? Illness And Health Update</a>
<a href=./apples-ipad-the-anandtech-review.html>The Silicon - Meet the A4 - Apple's iPad&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>